apiVersion: v1
kind: Service
metadata:
  name: job-master-svc
  namespace: ai
spec:
  selector:
    run: job-ml-master
  ports:
    - name: ssh
      protocol: TCP
      port: 22
      targetPort: 22
---
apiVersion: batch/v1
kind: Job
metadata:
  name: job-master # (optional)改变
  namespace: ai
spec:
  template:
    metadata:
      name: job-master
      labels:
        run: job-ml-master
    spec:
      containers:
      - name: job-master-container
        image: horovod/horovod:0.18.1-tf1.14.0-torch1.2.0-mxnet1.5.0-py3.6
        command: ["/bin/sh", "-c"]
        args: ["cd /usr/share/horovod/SSD-Tensorflow/; \
               apt update -y; \
               apt install ssh vim sshpass -y; \
               echo root:admin123|chpasswd; \
               tmp='PermitRootLogin yes'; \
               sed -i \"/^#PermitRootLogin/c$tmp\" /etc/ssh/sshd_config; \
               /etc/init.d/ssh restart; \
               ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa; \
               sshpass -p admin123 ssh-copy-id root@job-master-svc.ai.svc.cluster.local; \
               sshpass -p admin123 ssh-copy-id root@pod-slave1-svc.ai.svc.cluster.local; \
               horovodrun -np 2 -H job-master-svc.ai.svc.cluster.local:1,pod-slave1-svc.ai.svc.cluster.local:1 python myTrain_horovod_without_summary.py > result 2>&1"]
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: mount-name-testing
          mountPath: /usr/share/horovod
      restartPolicy: Never
      volumes:
        - name: mount-name-testing
          persistentVolumeClaim:
            claimName: pod-pvc-volume-1
      nodeSelector:
        disktype: gpu-node2
